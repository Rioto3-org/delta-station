#!/usr/bin/env python3
"""
Deltaåœ°ç‚¹ å®šç‚¹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

å–å¾—è¦³ç‚¹ï¼š
1. HTMLã®å–å¾—æˆåŠŸ
2. è¦³æ¸¬æ—¥æ™‚ã®æŠ½å‡ºï¼ˆobserved_atï¼‰
3. æ’®å½±æ—¥æ™‚ã®æŠ½å‡ºã¨å¤‰æ›ï¼ˆcaptured_atï¼‰
4. ä½æ‰€ã®æŠ½å‡ºï¼ˆlocation_addressï¼‰
5. è¦³æ¸¬åœ°ç‚¹åã®æŠ½å‡ºï¼ˆlocation_nameï¼‰
6. æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆç´¯åŠ é›¨é‡ã€æ°—æ¸©ã€é¢¨é€Ÿã€è·¯é¢æ¸©åº¦ã€è·¯é¢çŠ¶æ³ï¼‰
7. ç”»åƒURLã®æŠ½å‡ºã¨çµ¶å¯¾URLåŒ–
"""

import logging
import re
from datetime import datetime
from typing import Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class DeltaStationScraper:
    """Deltaåœ°ç‚¹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""

    def __init__(self, url: str):
        self.url = url
        self.soup: Optional[BeautifulSoup] = None
        self.data: Dict = {}

    def fetch_html(self) -> bool:
        """HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ1: HTMLã®å–å¾—ã‚’é–‹å§‹")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(self.url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding  # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®š
            self.soup = BeautifulSoup(response.text, 'lxml')
            logger.info(f"  â†’ æˆåŠŸ: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            logger.info(f"  â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def extract_observed_at(self) -> bool:
        """è¦³æ¸¬æ—¥æ™‚ã‚’æŠ½å‡º"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ2: è¦³æ¸¬æ—¥æ™‚ã®æŠ½å‡º")
        try:
            # "è¦³æ¸¬æ—¥æ™‚ï¼š2026-02-16 10:30" ã‚’æ¢ã™
            text = self.soup.get_text()
            match = re.search(r'è¦³æ¸¬æ—¥æ™‚[ï¼š:]\s*(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', text)
            if match:
                self.data['observed_at'] = match.group(1).strip()
                logger.info(f"  â†’ æˆåŠŸ: {self.data['observed_at']}")
                return True
            else:
                logger.error(f"  â†’ å¤±æ•—: è¦³æ¸¬æ—¥æ™‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def extract_captured_at(self) -> bool:
        """æ’®å½±æ—¥æ™‚ã‚’æŠ½å‡ºã—ã¦å¤‰æ›"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ3: æ’®å½±æ—¥æ™‚ã®æŠ½å‡ºã¨å¤‰æ›")
        try:
            # "æ’®å½±æ—¥æ™‚ï¼š02/16 10:32" ã‚’æ¢ã™
            text = self.soup.get_text()
            match = re.search(r'æ’®å½±æ—¥æ™‚[ï¼š:]\s*(\d{2}/\d{2})\s+(\d{2}:\d{2})', text)
            if match:
                # MM/DD HH:MM â†’ YYYY-MM-DD HH:MM ã«å¤‰æ›
                month_day = match.group(1)
                time = match.group(2)
                # è¦³æ¸¬æ—¥æ™‚ã‹ã‚‰å¹´ã‚’å–å¾—
                if 'observed_at' in self.data:
                    year = self.data['observed_at'][:4]
                else:
                    year = str(datetime.now().year)

                captured_str = f"{year}-{month_day.replace('/', '-')} {time}"
                self.data['captured_at'] = captured_str
                logger.info(f"  â†’ æˆåŠŸ: {match.group(1)} {time} â†’ {captured_str}")
                return True
            else:
                logger.error(f"  â†’ å¤±æ•—: æ’®å½±æ—¥æ™‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def extract_location_address(self) -> bool:
        """ä½æ‰€ã‚’æŠ½å‡º"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ4: ä½æ‰€ã®æŠ½å‡º")
        try:
            # class="style3" ã® div ã‚’æ¢ã™
            div = self.soup.find('div', class_='style3')
            if div:
                address = div.get_text().strip()
                self.data['location_address'] = address
                logger.info(f"  â†’ æˆåŠŸ: {address}")
                return True
            else:
                logger.error(f"  â†’ å¤±æ•—: ä½æ‰€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def extract_weather_data(self) -> bool:
        """æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ5-6: æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º")
        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰è¦³æ¸¬åœ°ç‚¹åã¨æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
            tables = self.soup.find_all('table')

            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) == 2:
                        label = cols[0].get_text().strip()
                        value = cols[1].get_text().strip()

                        if label == 'è¦³æ¸¬åœ°ç‚¹':
                            self.data['location_name'] = value
                            logger.info(f"  â†’ è¦³æ¸¬åœ°ç‚¹: {value}")
                        elif label == 'ç´¯åŠ é›¨é‡':
                            # "0mm" â†’ 0.0
                            num = re.search(r'([\d.]+)', value)
                            self.data['cumulative_rainfall'] = float(num.group(1)) if num else None
                            logger.info(f"  â†’ ç´¯åŠ é›¨é‡: {value} â†’ {self.data['cumulative_rainfall']}")
                        elif label == 'æ°—æ¸©':
                            # "5.0â„ƒ" â†’ 5.0
                            num = re.search(r'([\d.]+)', value)
                            self.data['temperature'] = float(num.group(1)) if num else None
                            logger.info(f"  â†’ æ°—æ¸©: {value} â†’ {self.data['temperature']}")
                        elif label == 'é¢¨é€Ÿ':
                            # "1.8m/s" â†’ 1.8
                            num = re.search(r'([\d.]+)', value)
                            self.data['wind_speed'] = float(num.group(1)) if num else None
                            logger.info(f"  â†’ é¢¨é€Ÿ: {value} â†’ {self.data['wind_speed']}")
                        elif label == 'è·¯é¢æ¸©åº¦':
                            # "8.2â„ƒ" â†’ 8.2
                            num = re.search(r'([\d.]+)', value)
                            self.data['road_temperature'] = float(num.group(1)) if num else None
                            logger.info(f"  â†’ è·¯é¢æ¸©åº¦: {value} â†’ {self.data['road_temperature']}")
                        elif label == 'è·¯é¢çŠ¶æ³':
                            self.data['road_condition'] = value
                            logger.info(f"  â†’ è·¯é¢çŠ¶æ³: {value}")

            # å¿…é ˆé …ç›®ã®ãƒã‚§ãƒƒã‚¯
            required = ['location_name', 'cumulative_rainfall', 'temperature',
                       'wind_speed', 'road_temperature', 'road_condition']
            missing = [k for k in required if k not in self.data]

            if missing:
                logger.error(f"  â†’ å¤±æ•—: æœªå–å¾—é …ç›® {missing}")
                return False

            logger.info(f"  â†’ æˆåŠŸ: ã™ã¹ã¦ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def extract_image_url(self) -> bool:
        """ç”»åƒURLã‚’æŠ½å‡ºã—ã¦çµ¶å¯¾URLã«å¤‰æ›"""
        logger.info(f"âœ“ ãƒ†ã‚¹ãƒˆ7: ç”»åƒURLã®æŠ½å‡º")
        try:
            # <img src="image/DR-74125-l.jpg" alt=""> ã‚’æ¢ã™
            img_tag = self.soup.find('img', src=re.compile(r'DR-\d+-l\.jpg'))
            if img_tag:
                relative_url = img_tag['src']
                absolute_url = urljoin(self.url, relative_url)
                self.data['image_url'] = absolute_url
                logger.info(f"  â†’ ç›¸å¯¾URL: {relative_url}")
                logger.info(f"  â†’ çµ¶å¯¾URL: {absolute_url}")

                # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆYYYYMMDD_HHMMSS_DR-74125.jpgï¼‰
                if 'observed_at' in self.data:
                    timestamp = self.data['observed_at'].replace('-', '').replace(':', '').replace(' ', '_')
                    filename_match = re.search(r'(DR-\d+-l)\.jpg', relative_url)
                    if filename_match:
                        base = filename_match.group(1)
                        filename = f"{timestamp}_{base}.jpg"
                        self.data['image_filename'] = filename
                        logger.info(f"  â†’ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")

                return True
            else:
                logger.error(f"  â†’ å¤±æ•—: ç”»åƒURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def run_test(self) -> Dict:
        """ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("=" * 60)
        logger.info("Deltaåœ°ç‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info("=" * 60)

        results = {}

        # ãƒ†ã‚¹ãƒˆ1: HTMLå–å¾—
        results['fetch_html'] = self.fetch_html()
        if not results['fetch_html']:
            logger.error("HTMLå–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ†ã‚¹ãƒˆã‚’ä¸­æ–­ã—ã¾ã™")
            return results

        # ãƒ†ã‚¹ãƒˆ2: è¦³æ¸¬æ—¥æ™‚
        results['observed_at'] = self.extract_observed_at()

        # ãƒ†ã‚¹ãƒˆ3: æ’®å½±æ—¥æ™‚
        results['captured_at'] = self.extract_captured_at()

        # ãƒ†ã‚¹ãƒˆ4: ä½æ‰€
        results['location_address'] = self.extract_location_address()

        # ãƒ†ã‚¹ãƒˆ5-6: æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        results['weather_data'] = self.extract_weather_data()

        # ãƒ†ã‚¹ãƒˆ7: ç”»åƒURL
        results['image_url'] = self.extract_image_url()

        # çµæœã‚µãƒãƒªãƒ¼
        logger.info("=" * 60)
        logger.info("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 60)
        success = sum(1 for v in results.values() if v)
        total = len(results)
        logger.info(f"æˆåŠŸ: {success}/{total}")

        for test_name, result in results.items():
            status = "âœ“ PASS" if result else "âœ— FAIL"
            logger.info(f"  {status}: {test_name}")

        # å–å¾—ãƒ‡ãƒ¼ã‚¿ä¸€è¦§
        if any(results.values()):
            logger.info("=" * 60)
            logger.info("å–å¾—ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")
            logger.info("=" * 60)
            for key, value in self.data.items():
                logger.info(f"  {key}: {value}")

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    url = "http://www2.thr.mlit.go.jp/sendai/html/DR-74125.html"

    scraper = DeltaStationScraper(url)
    results = scraper.run_test()

    # ã™ã¹ã¦æˆåŠŸã—ãŸã‹ç¢ºèª
    if all(results.values()):
        logger.info("\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸï¼")
        return 0
    else:
        logger.warning("\nâš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1


if __name__ == "__main__":
    exit(main())
