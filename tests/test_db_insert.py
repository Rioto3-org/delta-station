#!/usr/bin/env python3
"""
Deltaåœ°ç‚¹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥ãƒ†ã‚¹ãƒˆ

å®Ÿéš›ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥ã—ã€
æ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã€‚

ãƒ†ã‚¹ãƒˆå®Ÿæ–½æ—¥: 2026/02/16
"""

import logging
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from models import LocationData, ObservationData, ScrapedRawData

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, db_path: str = "test_delta_station.db"):
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None

    def initialize_database(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ï¼ˆã‚¹ã‚­ãƒ¼ãƒé©ç”¨ï¼‰"""
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–")
        try:
            schema_path = Path("database/schema.sql")
            if not schema_path.exists():
                logger.error(f"  â†’ å¤±æ•—: ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {schema_path}")
                return False

            with open(schema_path, 'r', encoding='utf-8') as f:
                schema_sql = f.read()

            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA foreign_keys = ON")  # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’æœ‰åŠ¹åŒ–
            self.conn.executescript(schema_sql)
            self.conn.commit()

            logger.info(f"  â†’ æˆåŠŸ: {self.db_path} ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def insert_location(self, location: LocationData) -> bool:
        """è¦³æ¸¬åœ°ç‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥"""
        logger.info("âœ“ è¦³æ¸¬åœ°ç‚¹ãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥")
        try:
            # æ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            cursor = self.conn.execute(
                "SELECT id FROM locations WHERE location_name = ?",
                (location.location_name,)
            )
            existing = cursor.fetchone()

            if existing:
                logger.info(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«ç™»éŒ²æ¸ˆã¿ (ID: {existing[0]})")
                return True

            # æŒ¿å…¥
            self.conn.execute(
                """
                INSERT INTO locations (id, location_name, location_address, source_url)
                VALUES (?, ?, ?, ?)
                """,
                (location.id, location.location_name, location.location_address, location.source_url)
            )
            self.conn.commit()

            logger.info(f"  â†’ æˆåŠŸ: ID={location.id}, åœ°ç‚¹å={location.location_name}")
            logger.info(f"  â†’ LocationData: {location.model_dump_json(ensure_ascii=False)}")
            return True
        except sqlite3.IntegrityError as e:
            logger.error(f"  â†’ å¤±æ•—: ä¸€æ„åˆ¶ç´„é•å {e}")
            return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def insert_observation(self, observation: ObservationData) -> bool:
        """è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥"""
        logger.info("âœ“ è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥")
        try:
            # æ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆobserved_at ãŒUNIQUEï¼‰
            cursor = self.conn.execute(
                "SELECT id FROM observations WHERE observed_at = ?",
                (observation.observed_at,)
            )
            existing = cursor.fetchone()

            if existing:
                logger.info(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«ç™»éŒ²æ¸ˆã¿ (ID: {existing[0]}, è¦³æ¸¬æ—¥æ™‚: {observation.observed_at})")
                return True

            # æŒ¿å…¥
            self.conn.execute(
                """
                INSERT INTO observations (
                    location_id, observed_at, captured_at,
                    cumulative_rainfall, temperature, wind_speed,
                    road_temperature, road_condition,
                    image_filename, image_url
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    observation.location_id,
                    observation.observed_at,
                    observation.captured_at,
                    observation.cumulative_rainfall,
                    observation.temperature,
                    observation.wind_speed,
                    observation.road_temperature,
                    observation.road_condition,
                    observation.image_filename,
                    observation.image_url
                )
            )
            self.conn.commit()

            logger.info(f"  â†’ æˆåŠŸ: è¦³æ¸¬æ—¥æ™‚={observation.observed_at}")
            logger.info(f"  â†’ ObservationData: {observation.model_dump_json(ensure_ascii=False)}")
            return True
        except sqlite3.IntegrityError as e:
            logger.error(f"  â†’ å¤±æ•—: ä¸€æ„åˆ¶ç´„é•åã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼ {e}")
            return False
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def verify_data(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãæŒ¿å…¥ã•ã‚ŒãŸã‹ç¢ºèª"""
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿ç¢ºèª")
        try:
            # è¦³æ¸¬åœ°ç‚¹æ•°ã‚’ç¢ºèª
            cursor = self.conn.execute("SELECT COUNT(*) FROM locations")
            location_count = cursor.fetchone()[0]
            logger.info(f"  â†’ locations ãƒ†ãƒ¼ãƒ–ãƒ«: {location_count} ä»¶")

            # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ç¢ºèª
            cursor = self.conn.execute("SELECT COUNT(*) FROM observations")
            observation_count = cursor.fetchone()[0]
            logger.info(f"  â†’ observations ãƒ†ãƒ¼ãƒ–ãƒ«: {observation_count} ä»¶")

            # æœ€æ–°ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦è¡¨ç¤º
            cursor = self.conn.execute(
                """
                SELECT
                    o.id, l.location_name, o.observed_at, o.captured_at,
                    o.cumulative_rainfall, o.temperature, o.wind_speed,
                    o.road_temperature, o.road_condition,
                    o.image_filename
                FROM observations o
                JOIN locations l ON o.location_id = l.id
                ORDER BY o.observed_at DESC
                LIMIT 1
                """
            )
            latest = cursor.fetchone()

            if latest:
                logger.info("  â†’ æœ€æ–°ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿:")
                logger.info(f"      ID: {latest[0]}")
                logger.info(f"      è¦³æ¸¬åœ°ç‚¹: {latest[1]}")
                logger.info(f"      è¦³æ¸¬æ—¥æ™‚: {latest[2]}")
                logger.info(f"      æ’®å½±æ—¥æ™‚: {latest[3]}")
                logger.info(f"      ç´¯åŠ é›¨é‡: {latest[4]} mm")
                logger.info(f"      æ°—æ¸©: {latest[5]} â„ƒ")
                logger.info(f"      é¢¨é€Ÿ: {latest[6]} m/s")
                logger.info(f"      è·¯é¢æ¸©åº¦: {latest[7]} â„ƒ")
                logger.info(f"      è·¯é¢çŠ¶æ³: {latest[8]}")
                logger.info(f"      ç”»åƒ: {latest[9]}")

            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self.conn:
            self.conn.close()


class DeltaStationScraper:
    """Deltaåœ°ç‚¹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆtest_scraper.pyã‹ã‚‰ç§»æ¤ï¼‰"""

    def __init__(self, url: str, image_dir: str = "images"):
        self.url = url
        self.soup: Optional[BeautifulSoup] = None
        self.image_dir = Path(image_dir)
        self.image_dir.mkdir(exist_ok=True)

    def fetch_html(self) -> bool:
        """HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹"""
        logger.info("âœ“ HTMLã®å–å¾—")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(self.url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            self.soup = BeautifulSoup(response.text, 'lxml')
            logger.info(f"  â†’ æˆåŠŸ: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False

    def scrape(self) -> Optional[ScrapedRawData]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
        try:
            data = {}

            # è¦³æ¸¬æ—¥æ™‚
            text = self.soup.get_text()
            match = re.search(r'è¦³æ¸¬æ—¥æ™‚[ï¼š:]\s*(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', text)
            if match:
                data['observed_at'] = match.group(1).strip()
                logger.info(f"  â†’ è¦³æ¸¬æ—¥æ™‚: {data['observed_at']}")

            # æ’®å½±æ—¥æ™‚
            match = re.search(r'æ’®å½±æ—¥æ™‚[ï¼š:]\s*(\d{2}/\d{2})\s+(\d{2}:\d{2})', text)
            if match:
                year = data['observed_at'][:4] if 'observed_at' in data else str(datetime.now().year)
                month_day = match.group(1)
                time = match.group(2)
                data['captured_at'] = f"{year}-{month_day.replace('/', '-')} {time}"
                logger.info(f"  â†’ æ’®å½±æ—¥æ™‚: {data['captured_at']}")

            # ä½æ‰€
            div = self.soup.find('div', class_='style3')
            if div:
                data['location_address'] = div.get_text().strip()
                logger.info(f"  â†’ ä½æ‰€: {data['location_address']}")

            # æ°—è±¡ãƒ‡ãƒ¼ã‚¿
            tables = self.soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) == 2:
                        label = cols[0].get_text().strip()
                        value = cols[1].get_text().strip()

                        if label == 'è¦³æ¸¬åœ°ç‚¹':
                            data['location_name'] = value
                        elif label == 'ç´¯åŠ é›¨é‡':
                            data['cumulative_rainfall'] = value
                        elif label == 'æ°—æ¸©':
                            data['temperature'] = value
                        elif label == 'é¢¨é€Ÿ':
                            data['wind_speed'] = value
                        elif label == 'è·¯é¢æ¸©åº¦':
                            data['road_temperature'] = value
                        elif label == 'è·¯é¢çŠ¶æ³':
                            data['road_condition'] = value

            # ç”»åƒURL
            img_tag = self.soup.find('img', src=re.compile(r'DR-\d+-l\.jpg'))
            if img_tag:
                relative_url = img_tag['src']
                data['image_url'] = urljoin(self.url, relative_url)
                logger.info(f"  â†’ ç”»åƒURL: {data['image_url']}")

            logger.info("  â†’ æˆåŠŸ: ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")

            # ScrapedRawDataã«å¤‰æ›
            return ScrapedRawData(**data)
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return None

    def download_image(self, image_url: str, image_filename: str) -> bool:
        """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜"""
        logger.info("âœ“ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        try:
            image_path = self.image_dir / image_filename

            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if image_path.exists():
                logger.info(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«å­˜åœ¨ã—ã¾ã™ ({image_filename})")
                return True

            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(image_url, headers=headers, timeout=30)
            response.raise_for_status()

            # ä¿å­˜
            with open(image_path, 'wb') as f:
                f.write(response.content)

            file_size = len(response.content)
            logger.info(f"  â†’ æˆåŠŸ: {image_filename} ({file_size:,} bytes)")
            logger.info(f"  â†’ ä¿å­˜å…ˆ: {image_path}")
            return True
        except Exception as e:
            logger.error(f"  â†’ å¤±æ•—: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("Deltaåœ°ç‚¹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥ãƒ†ã‚¹ãƒˆ")
    logger.info("ãƒ†ã‚¹ãƒˆå®Ÿæ–½æ—¥: 2026/02/16")
    logger.info("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    db = DatabaseManager()
    if not db.initialize_database():
        logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1

    # è¦³æ¸¬åœ°ç‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆNo.1: ä½œä¸¦å®¿ï¼‰
    location = LocationData(
        id=1,
        location_name="ä½œä¸¦å®¿",
        location_address="ä»™å°å¸‚é’è‘‰åŒºä½œä¸¦å­—ç¥å‰è¥¿",
        source_url="http://www2.thr.mlit.go.jp/sendai/html/DR-74125.html"
    )

    # è¦³æ¸¬åœ°ç‚¹ã‚’æŒ¿å…¥
    if not db.insert_location(location):
        logger.error("è¦³æ¸¬åœ°ç‚¹ã®æŒ¿å…¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
        db.close()
        return 1

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraper = DeltaStationScraper(location.source_url)
    if not scraper.fetch_html():
        logger.error("HTMLå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        db.close()
        return 1

    raw_data = scraper.scrape()
    if not raw_data:
        logger.error("ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
        db.close()
        return 1

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    timestamp = raw_data.observed_at.replace('-', '').replace(':', '').replace(' ', '_')
    image_filename = f"{timestamp}_DR-74125-l.jpg"

    # ObservationDataã«å¤‰æ›ï¼ˆPydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼‰
    logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
    try:
        observation = raw_data.to_observation(
            location_id=location.id,
            image_filename=image_filename
        )
        logger.info("  â†’ æˆåŠŸ: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
    except Exception as e:
        logger.error(f"  â†’ å¤±æ•—: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ {e}")
        db.close()
        return 1

    # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if not scraper.download_image(observation.image_url, observation.image_filename):
        logger.warning("ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
    if not db.insert_observation(observation):
        logger.error("è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
        db.close()
        return 1

    # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    if not db.verify_data():
        logger.error("ãƒ‡ãƒ¼ã‚¿ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸ")
        db.close()
        return 1

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    db.close()

    # ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    image_count = len(list(scraper.image_dir.glob("*.jpg")))

    logger.info("=" * 60)
    logger.info("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸï¼")
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {db.db_path}")
    logger.info(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {scraper.image_dir} ({image_count} ä»¶)")
    logger.info("=" * 60)
    return 0


if __name__ == "__main__":
    exit(main())
